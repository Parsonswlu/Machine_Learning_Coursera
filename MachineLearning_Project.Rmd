---
title: "Coursera Machine Learning Project"
author: "Andrew Parsons"
date: "Sept. 22, 2015"
output: pdf_document
---

###Abstract

Using accelerometer data taken from six participants doing a variety of exercises, classification tree and random forest models were trained to predict which activity was being done given only the accelerometer data. Ultimately the random forest model was chosen for forecasting, as it had the highest prediction accuracy on the cross validation set (99.5%).

###Background

Six participants were asked to wear devices on their belt, forearm, arm and dumbell while performing barbell lifts correctly and incorrectly in five different ways. The data provided are readings from the accelerometers. The goal of the exercise is to use this data to ascertain which of the five different ways a user is doing the exercise based on new accelerometer data.

###Initializing the data

The first step is to load the apprpriate libraries and download and read the data from the [provided source of accelerometer data](http://groupware.les.inf.puc-rio.br/har).

```{r initialize_data, echo=TRUE, eval=TRUE,message=FALSE}
library(caret); library(knitr); library(rpart); library(rpart.plot); library(randomForest)
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

buildData <- read.csv(url(url_train), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(url_test), na.strings=c("NA","#DIV/0!",""))
```

###Examining the Data

Once the data is loaded it is examined to get an idea of what is being worked with.

```{r examine_data1, echo=TRUE,eval=TRUE}
dim(buildData) 
dim(testing)
```

The data initially contains `r length(buildData)` variates (including the one being predicted on - 'classe'). To simplify things, some data cleaning can be done so the machine learning model can provide a better result.

The first 7 columns are descriptive - i.e. the row index, the name of the participant, the time the exercise occurred, etc. These can be removed as they should have no robust predictive value.

There are also a large number of columns (`r sum(as.logical(!(colSums(is.na(buildData)) == 0)))`) that consist entirely (or almost entirely) of NA's. These are also removed for purposes of prediction.

```{r examine_data2, echo=TRUE,eval=TRUE}
# First 7 columns are descriptive and not useful for prediction
summary(buildData[,1:7])

# 100 variables contain almost entirely NAs
hist(colSums(is.na(buildData)))

# Remove first 7 columns
buildData <- buildData[,8:length(buildData)]
testing <- testing[,8:length(testing)]

# Remove Columns that contain almost entirely NA's
columns_with_NAs <- as.logical(!(colSums(is.na(buildData)) == 0))
buildData <- buildData[!columns_with_NAs]
testing <- testing[!columns_with_NAs]
```

Now the data set is down to a more reasonable size of `r length(buildData)` (including the predictor 'classe').

###Split the Data

To determine which model is expect to perform best on the test set, the model will be trained on a training set then checked against a validation set. The data is split 70% training / 30% validation for this purpose. Seed is set as 5555 for reproducibility purposes.

```{r split_data, echo=TRUE, eval=TRUE}
set.seed(5555)
inTrain <- createDataPartition(buildData$classe, p = 0.7, list = FALSE)
training <- buildData[inTrain,]
validation <- buildData[-inTrain,]
```

###Model 1 - Classification Tree Model

For the purpose of classifying the 'classe' variable into one of five categories, the most logical first step would be to try a straightforward classification tree using the `r length(buildData)-1` predictors. The training, visualization of the classification tree and results from a confusion matrix are shown below.

```{r classification_tree, echo=TRUE, eval=TRUE, cache = TRUE, fig.height = 8, fig.width=12}
# Classification Tree Training
fit_rpart <- rpart(classe ~ ., data = training, method = "class")

# Classification Tree Visualization
rpart.plot(fit_rpart)

# Classification Tree Prediction results and Confusion Matrix
predict_rpart <- predict(fit_rpart, newdata = validation, type = "class")
confusionMatrix(predict_rpart, validation$classe)
```

Prediction using this model on the validation set provides an overall accuracy of `r format(confusionMatrix(predict_rpart,validation$classe)$overall[1]*100,digits=3)`%, which is good but not great. Note however that this classification model took very little processing time to execute, and in a pinch could be used if results were needed very quickly.

###Model 2 - Random Forests

Random forests are more complex than classification trees and offer additional benefits, including robustness to outliers. This comes at a tradeoff of increased processing time and reduced interpretability.

To help minimize the processing time for the random forest model, it may help to tune the model by determining ahead of time an ideal number of variables and trees to use. The tuneRF function below shows that the optimal number of variables to use in this case was 7 and that 250 trees was plenty to minimize the out-of-bag error estimate for each category of exercise. 

```{r random_forest, echo=TRUE, eval=TRUE, cache=TRUE,fig.height = 6, fig.width=12}
# Try tuning Random Forest - optimal OOB error uses 7 variables
tune <- tuneRF(training[,-53], training$classe)

# Random Forest Training with 7 variables and 250 trees
fit_rf <- randomForest(classe ~ ., data = training, ntree = 250, mtry = 7,
                       prox=TRUE, keep.forest = TRUE)

# OOB error estimate per number of trees
plot(fit_rf, log="y", type='l', main="Out-of-bag error estimate per number of trees")
legend("topright", legend = colnames(fit_rf$err.rate), col=1:6, fill=1:6)

# Random Forest Prediction results and Confusion Matrix
predict_rf <- predict(fit_rf, newdata = validation)
confusionMatrix(predict_rf, validation$classe)
```

Prediction using the random forest model provides an overall accuracy of `r format(confusionMatrix(predict_rf,validation$classe)$overall[1]*100,digits=3)`% on the validation set. Note the improved accuracy came at a significant increase in processing time and a decreased intepretability as to why the chosen parameters were chosen.

###Random Forest Prediction on Testing Data

Given the high accuracy of the random forest model on the validation set, this model is chosen to be used on the test set of 20 cases using the following lines of code:

```{r test_code, echo=TRUE, eval=TRUE}
answers = predict(fit_rf, newdata = testing)

pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(answers)
```

Assuming the same out-of-sample error rate as was seen on the validation set (~0.5%), it is expected that the program will accurately predict all 20 exercises.

###Summary

Given a set of (cleaned) accelerometer data, classification tree and random forest models were trained and validated against to try and predict out-of-sample accuracy. The classification tree resulted in an okay prediction result of `r format(confusionMatrix(predict_rpart,validation$classe)$overall[1]*100,digits=3)`%, however the random forest model had a much higher accuracy rate of `r format(confusionMatrix(predict_rf,validation$classe)$overall[1]*100,digits=3)`%. Even with the increased processing time, the random forest model was greatly superior in predicting out-of-sample performance on the test set.

##Appendix

###Systems Used

Program created in RStudio using R version 3.2.2 (2015-08-14) -- "Fire Safety" 

Windows 8.1 

Dell Laptop, 64-bit operating system 

###Data source

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

